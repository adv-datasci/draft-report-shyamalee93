---
title: "Project Plan"
author: "Shyamalee"
date: "September 22, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Zillow Prize - Project for Advanced Data Science

The Zillow Prize Competition on Kaggle: The participants are expected to predict future sale prices of homes, to make the mean error on their already existing Zestimate even smaller. 
The submissions are evaluated on Mean absolute error defined as:

$logerror=log(Zestimate)???log(SalePrice)$

Our submission must have a log error at six different points in time for each unique property. 

## Project Plan

1. Start with looking at the dataset in R
2. Data files :
  + Properties.csv : Contains a list of all properties and 58 features for each properties
  + train_2016 : Contains the training dataset which has the parcelID (Unique house identifier), transaction date and mean log error between actual sale price and Zestimate
  + Submission file - Contains mean log error across six time periods for every individual parcel ID
3. Do some Exploratory Data Analysis on properties.csv 
  + Figure out the categorical and continuous variables
  + Plot the distributions of continuous variables
  + Check variables have over 75-80 percent missing values, check if they can be imputed or discard them
  + Check if more variables can be determined: Ratios etc. 
4. Check out the Datacamp module on Machine Learning, start building a basic algorithm

## Revised Analysis Plan

1. Start with an Expolratory Data Analysis: 

  + Look at all the 58 features in the properties dataset 
  + Plot some exploratory graphs, the distribution of transaction dates from our train_2016.csv, distribution of log error, how the log error changes over time, when the houses were built, how log error changes over build year? 
  + Check distribution of log error with total tax, number of rooms, numbre of units, different types of areas etc
  + Check missing values - how many features have over 50 percent missing values?
  + Check correlation between variables (Avoid multicollinearity)
  + Omit fields with extremely high missing values, let me arbitrarily keep that at 80 percent
  + Other fields that have less than eighty percent missing values, impute missing values by using a k-nearest neighbor as the houses are geographically similar and must be close to each other (How to find optimal K-value?)
  + For highly correlated features, check if one with lower missing values can be used to impute the one with the higher number of missing values
  + Followup: What John Mentioned, make a binary column,, are you missing values or not? Yes or No. Add this to the prediction model to check if that column might be important

2. Add new features that you might think are necessary: 
  + How old is your property? Current Date - Year the house was built
  + How much is the total living area in the house? Finished Sq.ft/ Total Area
  + Total Number of Rooms - Number of bedrooms + Bathroom count
  + Binary variables - Does the house have a pool/ hot tub? 
  + Continued - Is there air conditioning? Yes/ No? 
  + How long have taxes been unpaid? Current date - Tax Deliquency Year 
  + (Check if you can do/ create additional variables with the tax information)
  
3. What next after EDA and adding features? 
  + Merge the two datasets, properties and training data that has the log error
  + Use Multiple Machine Learning Algorithms that are commonly used to improve predictions in similar Kaggle competitions
  + Read up - Pakage xgboost. What is gradient boosting? 
  + Hope your logerror is reduced when you do apply one of the commonly used algorithms! 
  
  
+ Side - Continue with datacamp ML modules in R
  
  